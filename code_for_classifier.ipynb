{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.pylab import plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training data from 'multimodal_train.tsv' using tab as the separator\n",
    "df = pd.read_csv('multimodal_train.tsv', sep='\\t')\n",
    "\n",
    "# Selecting only the columns '2_way_label' and 'title' from the training DataFrame\n",
    "df = df[[\"2_way_label\", \"title\"]]\n",
    "\n",
    "# Renaming the '2_way_label' column to 'label' for better representation\n",
    "df = df.rename(columns={'2_way_label': 'label'})\n",
    "\n",
    "# Reading the test data from 'multimodal_test_public.tsv' using tab as the separator\n",
    "df_test = pd.read_csv('multimodal_test_public.tsv', sep='\\t')\n",
    "\n",
    "# Selecting only the columns '2_way_label' and 'title' from the test DataFrame\n",
    "df_test = df_test[[\"2_way_label\", \"title\"]]\n",
    "\n",
    "# Renaming the '2_way_label' column to 'label' in the test DataFrame for consistency\n",
    "df_test = df_test.rename(columns={'2_way_label': 'label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9182924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the labels from the training DataFrame\n",
    "y = df.label\n",
    "\n",
    "# Displaying the normalized value counts of labels in the training data\n",
    "y.value_counts(normalize=True).rename({1: 'real', 0: 'fake'})\n",
    "\n",
    "# Extracting the labels from the test DataFrame\n",
    "y_testing = df_test.label\n",
    "\n",
    "# Displaying the normalized value counts of labels in the test data\n",
    "y_testing.value_counts(normalize=True).rename({1: 'real', 0: 'fake'})\n",
    "\n",
    "# Defining class names for better interpretation\n",
    "class_names = ['fake', 'real']\n",
    "\n",
    "# Counting the occurrences of each label in the training data\n",
    "label_count = df.label.value_counts()\n",
    "\n",
    "# Creating a bar plot to visualize the distribution of fake and real news\n",
    "sns.barplot(x=label_count.index, y=label_count)\n",
    "\n",
    "# Setting the title for the plot\n",
    "plt.title('Distribution of Fake/Real News', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with null values in the training DataFrame\n",
    "df = df.dropna()\n",
    "\n",
    "# Dropping rows with null values in the test DataFrame\n",
    "df_test = df_test.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a948b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the 'title' column as the feature for training data\n",
    "X_train = df['title']\n",
    "\n",
    "# Extracting the 'title' column as the feature for test data\n",
    "X_test = df_test['title']\n",
    "\n",
    "# Extracting the 'label' column as the target for training data\n",
    "y_train = df['label']\n",
    "\n",
    "# Extracting the 'label' column as the target for test data\n",
    "y_test = df_test['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom tokenizer function that extracts lowercase words from the text\n",
    "def custom_tokenizer(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    tokens = [token.lower() for token in tokens]  \n",
    "    return tokens\n",
    "\n",
    "# Create a CountVectorizer with the custom tokenizer, using n-grams of size 1 to 3 and excluding English stop words\n",
    "count_vectorizer = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 3), stop_words='english')\n",
    "\n",
    "# Fit and transform the training data using the CountVectorizer\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the already fitted CountVectorizer\n",
    "count_test = count_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot a graph with training and test scores\n",
    "def plot_graph(x, train_score, test_score, x_label, title):\n",
    "    plt.plot(x, train_score, color='green', label='Train')\n",
    "    plt.plot(x, test_score, color='red', label='Test')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store training and test scores\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "# Define a range of alpha values\n",
    "alpha = np.arange(0.1, 1, 0.1)\n",
    "\n",
    "# Loop through each alpha value and train a Multinomial Naive Bayes model\n",
    "for index in alpha:\n",
    "    # Create a Multinomial Naive Bayes model with the specified alpha\n",
    "    model = MultinomialNB(alpha=index)\n",
    "    \n",
    "    # Fit the model using the training data\n",
    "    model.fit(count_train, y_train)\n",
    "    \n",
    "    # Predict labels for the training data\n",
    "    y_hat_train = model.predict(count_train)\n",
    "    \n",
    "    # Calculate the confusion matrix for the training data\n",
    "    cm_train = metrics.confusion_matrix(y_train, y_hat_train)\n",
    "    \n",
    "    # Calculate and append the training accuracy to the list\n",
    "    train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "    \n",
    "    # Predict labels for the test data\n",
    "    y_hat_test = model.predict(count_test)\n",
    "    \n",
    "    # Calculate the confusion matrix for the test data\n",
    "    cm_test = metrics.confusion_matrix(y_test, y_hat_test)\n",
    "    \n",
    "    # Calculate and append the test accuracy to the list\n",
    "    test_score.append(cm_test.diagonal().sum() / cm_test.sum())\n",
    "\n",
    "# Plot the graph with the obtained training and test scores\n",
    "plot_graph(alpha, train_score, test_score, \"Alpha\", \"MultinomialNB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a242ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store training and test scores\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "# Define a range of max_depth values\n",
    "max_depth = np.arange(2, 30)\n",
    "\n",
    "# Loop through each max_depth value and train a Decision Tree model\n",
    "for index in max_depth:\n",
    "    # Create a Decision Tree model with the specified max_depth and criterion='entropy'\n",
    "    model = DecisionTreeClassifier(criterion='entropy', max_depth=index)\n",
    "    \n",
    "    # Fit the model using the training data\n",
    "    model.fit(count_train, y_train)\n",
    "    \n",
    "    # Predict labels for the training data\n",
    "    y_hat_train = model.predict(count_train)\n",
    "    \n",
    "    # Calculate the confusion matrix for the training data\n",
    "    cm_train = metrics.confusion_matrix(y_train, y_hat_train)\n",
    "    \n",
    "    # Calculate and append the training accuracy to the list\n",
    "    accuracy_train = cm_train.diagonal().sum() / cm_train.sum()\n",
    "    train_score.append(accuracy_train)\n",
    "    \n",
    "    # Predict labels for the test data\n",
    "    y_hat_test = model.predict(count_test)\n",
    "    \n",
    "    # Calculate the confusion matrix for the test data\n",
    "    cm_test = metrics.confusion_matrix(y_test, y_hat_test)\n",
    "    \n",
    "    # Calculate and append the test accuracy to the list\n",
    "    accuracy_test = cm_test.diagonal().sum() / cm_test.sum()\n",
    "    test_score.append(accuracy_test)\n",
    "\n",
    "# Plot the graph with the obtained training and test scores\n",
    "plot_graph(max_depth, train_score, test_score, 'Max Depth', 'Decision Tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store training and test scores\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "# Create an XGBoost Classifier with a specified learning rate\n",
    "model = xgb.XGBClassifier(learning_rate=0.1)\n",
    "\n",
    "# Fit the model using the training data\n",
    "model.fit(count_train, y_train)\n",
    "\n",
    "# Predict labels for the training data\n",
    "y_hat_train = model.predict(count_train)\n",
    "\n",
    "# Calculate the confusion matrix for the training data\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "\n",
    "# Calculate and append the training accuracy to the list\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "\n",
    "# Predict labels for the test data\n",
    "y_hat_test = model.predict(count_test)\n",
    "\n",
    "# Calculate the confusion matrix for the test data\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "\n",
    "# Calculate and append the test accuracy to the list\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa210735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.2)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.3)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.4)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fac5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.5)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b68c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.6)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.7)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2537e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.8)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3abccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(learning_rate=0.9)  # Corrected learning_rate parameter\n",
    "model.fit(count_train, y_train)\n",
    "y_hat_train = model.predict(count_train)\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "y_hat_test = model.predict(count_test)\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "test_score.append(cm_test.diagonal().sum() / cm_test.sum())\n",
    "plot_graph(np.arange(0.1,1,0.1),train_score,test_score,\"alpha\",\"XGBClassifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69f6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store training and test scores\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "# Define a range of C values\n",
    "C_values = np.arange(0.1, 1, 0.1)\n",
    "\n",
    "# Loop through each C value and train a Logistic Regression model\n",
    "for C_val in C_values:\n",
    "    # Create a Logistic Regression model with the specified C value and maximum iterations\n",
    "    model = LogisticRegression(C=C_val, max_iter=500)\n",
    "    \n",
    "    # Fit the model using the training data\n",
    "    model.fit(count_train, y_train)\n",
    "    \n",
    "    # Predict labels for the training data\n",
    "    y_hat_train = model.predict(count_train)\n",
    "    \n",
    "    # Calculate the confusion matrix for the training data\n",
    "    cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "    \n",
    "    # Calculate and append the training accuracy to the list\n",
    "    train_score.append(cm_train.diagonal().sum() / cm_train.sum())\n",
    "    \n",
    "    # Predict labels for the test data\n",
    "    y_hat_test = model.predict(count_test)\n",
    "    \n",
    "    # Calculate the confusion matrix for the test data\n",
    "    cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "    \n",
    "    # Calculate and append the test accuracy to the list\n",
    "    test_score.append(cm_test.diagonal().sum() / cm_test.sum())\n",
    "\n",
    "# Plot the graph with the obtained training and test scores\n",
    "plot_graph(C_values, train_score, test_score, \"C\", \"LogisticRegression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model with regularization parameter C=0.9 and maximum iterations set to 500\n",
    "model = LogisticRegression(C=0.9, max_iter=500)\n",
    "\n",
    "# Fit the model using the training data and its corresponding labels\n",
    "model.fit(count_train, y_train)\n",
    "\n",
    "# Predict labels for the training data\n",
    "y_hat_train = model.predict(count_train)\n",
    "\n",
    "# Calculate the confusion matrix for the training data\n",
    "cm_train = confusion_matrix(y_train, y_hat_train)\n",
    "\n",
    "# Calculate the training accuracy\n",
    "train_score = (cm_train.diagonal().sum() / cm_train.sum())\n",
    "\n",
    "# Predict labels for the test data\n",
    "y_hat_test = model.predict(count_test)\n",
    "\n",
    "# Calculate the confusion matrix for the test data\n",
    "cm_test = confusion_matrix(y_test, y_hat_test)\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_score = (cm_test.diagonal().sum() / cm_test.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained Logistic Regression model to a file using joblib\n",
    "dump(model, 'Classifier.joblib')\n",
    "\n",
    "# Save the fitted CountVectorizer to a file using joblib\n",
    "dump(count_vectorizer, 'Count_Vectorizer.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d985ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
